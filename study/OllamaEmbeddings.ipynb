{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 基本概念\n",
    "1. 定义\n",
    "- LangChain中的嵌入模型接口\n",
    "- 基于Ollama的文本向量化工具\n",
    "- 支持多种底层嵌入模型\n",
    "2. 主要功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# 基本初始化\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",  # 使用的模型\n",
    "    base_url=\"http://localhost:11434\"  # Ollama服务地址\n",
    ")\n",
    "\n",
    "#作用：\n",
    "# base_url 告诉程序在哪里找到运行的 Ollama 服务\n",
    "# 默认情况下，Ollama 服务在本地运行，监听 11434 端口\n",
    "# 通过这个 URL，程序可以向 Ollama 服务发送请求，获取文本嵌入\n",
    "# 参数组成：\n",
    "# http://localhost：表示服务在本地运行\n",
    "# 11434：是 Ollama 的默认端口号\n",
    "# 这个端口号是在安装 Ollama 时自动配置的\n",
    "\n",
    "# 生成文本嵌入\n",
    "text = \"这是一段示例文本\"\n",
    "text1 = \"这是一段示例文本1\"\n",
    "text2 = \"这是一段示例文本2\"\n",
    "vector = embeddings.embed_query(text)  # 生成单个文本的向量\n",
    "vectors = embeddings.embed_documents([text1, text2])  # 批量生成向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 工作原理\n",
    "1. 向量化过程\n",
    "   - 文本预处理\n",
    "   - 模型编码\n",
    "   - 向量生成\n",
    "   - 维度规范化\n",
    "2. 技术细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义配置示例\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",\n",
    "    num_ctx=4096,         # 上下文窗口大小\n",
    "    num_thread=4,         # 使用的线程数\n",
    "    show_progress=True,   # 显示进度\n",
    "    timeout=30            # 超时设置\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用场景\n",
    "1. 文本相似度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算两段文本的相似度\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def compute_similarity(text1, text2):\n",
    "    vec1 = embeddings.embed_query(text1)\n",
    "    vec2 = embeddings.embed_query(text2)\n",
    "    return dot(vec1, vec2)/(norm(vec1)*norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 向量数据库集成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 创建向量存储\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  3.语义搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现语义搜索\n",
    "def semantic_search(query, documents, top_k=3):\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    doc_vectors = embeddings.embed_documents(documents)\n",
    "    \n",
    "    similarities = [\n",
    "        dot(query_vector, doc_vec)/(norm(query_vector)*norm(doc_vec))\n",
    "        for doc_vec in doc_vectors\n",
    "    ]\n",
    "    \n",
    "    return sorted(\n",
    "        zip(documents, similarities),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 高级用法\n",
    "1. 批量处理优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量处理文档\n",
    "def process_documents_in_batches(documents, batch_size=32):\n",
    "    results = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        vectors = embeddings.embed_documents(batch)\n",
    "        results.extend(vectors)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.错误处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带重试的嵌入生成\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def embed_with_retry(text):\n",
    "    try:\n",
    "        return embeddings.embed_query(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.异步处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异步嵌入生成\n",
    "async def async_embed_documents(documents):\n",
    "    tasks = []\n",
    "    for doc in documents:\n",
    "        task = embeddings.aembed_query(doc)\n",
    "        tasks.append(task)\n",
    "    return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 性能优化建议\n",
    "1. 模型选择\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同场景的模型选择\n",
    "embeddings_fast = OllamaEmbeddings(model=\"bge-small-en\")  # 轻量快速\n",
    "embeddings_accurate = OllamaEmbeddings(model=\"bge-m3\")    # 高精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 缓存机制\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现简单的缓存\n",
    "from functools import lru_cache\n",
    "\n",
    "class CachedEmbeddings:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.model = embedding_model\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def embed_query(self, text):\n",
    "        return self.model.embed_query(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 资源管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 资源使用监控\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "def monitor_resources():\n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    memory_percent = psutil.virtual_memory().percent\n",
    "    if GPUtil.getGPUs():\n",
    "        gpu = GPUtil.getGPUs()[0]\n",
    "        gpu_load = gpu.load\n",
    "        gpu_memory = gpu.memoryUtil\n",
    "        return {\n",
    "            \"cpu\": cpu_percent,\n",
    "            \"memory\": memory_percent,\n",
    "            \"gpu_load\": gpu_load,\n",
    "            \"gpu_memory\": gpu_memory\n",
    "        }\n",
    "    return {\n",
    "        \"cpu\": cpu_percent,\n",
    "        \"memory\": memory_percent\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 实际应用示例\n",
    "1. 文档检索系统\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRetrieval:\n",
    "    def __init__(self, documents):\n",
    "        self.embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "    \n",
    "    def search(self, query, k=3):\n",
    "        return self.vectorstore.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 文本聚类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_texts(texts, n_clusters=5):\n",
    "    # 生成嵌入向量\n",
    "    vectors = embeddings.embed_documents(texts)\n",
    "    \n",
    "    # 执行聚类\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    clusters = kmeans.fit_predict(vectors)\n",
    "    \n",
    "    # 整理结果\n",
    "    results = {}\n",
    "    for text, cluster in zip(texts, clusters):\n",
    "        if cluster not in results:\n",
    "            results[cluster] = []\n",
    "        results[cluster].append(text)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "OllamaEmbeddings是一个强大的文本向量化工具，通过合理使用可以支持多种高级AI应用的开发。它的性能和易用性使其成为构建语义搜索、文档检索等应用的理想选择。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
