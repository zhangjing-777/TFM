{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 基本概念\n",
    "1.定义\n",
    "- LangChain中的检索式问答链\n",
    "- 结合文档检索和LLM的问答系统\n",
    "- 支持上下文增强的回答生成\n",
    "2. 核心组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 基本初始化\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=Ollama(model=\"llama2\"),\n",
    "    chain_type=\"stuff\",  # 链类型\n",
    "    retriever=vectorstore.as_retriever(),  # 检索器\n",
    "    return_source_documents=True  # 返回源文档\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 基本作用\n",
    "from_chain_type是RetrievalQA的工厂方法，用于创建不同类型的检索问答链。\n",
    "2. 参数详解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    # 必需参数\n",
    "    llm=llm,                    # 语言模型实例\n",
    "    chain_type=\"stuff\",         # 链类型\n",
    "    retriever=retriever,        # 检索器实例\n",
    "\n",
    "    # 可选参数\n",
    "    return_source_documents=True,  # 是否返回源文档\n",
    "    chain_type_kwargs={           # 链类型特定参数\n",
    "        \"prompt\": custom_prompt,\n",
    "        \"verbose\": True\n",
    "    },\n",
    "    verbose=True,                # 是否显示详细日志\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain_type\n",
    "- \"stuff\": 直接将所有文档合并，适合处理少量文档\n",
    "- \"map_reduce\": 分步处理大量文档\n",
    "- \"refine\": 逐步细化答案，适合需要高质量答案的场景\n",
    "- \"map_rerank\": 对答案进行重新排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 工作原理\n",
    "1. 检索过程\n",
    "2. 问答生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检索流程示例\n",
    "class CustomRetrievalQA:\n",
    "    def __init__(self, llm, retriever):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "    \n",
    "    def process_query(self, query):\n",
    "        # 1. 检索相关文档\n",
    "        relevant_docs = self.retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # 2. 构建上下文\n",
    "        context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "        \n",
    "        # 3. 构建提示词\n",
    "        prompt = f\"\"\"\n",
    "        基于以下信息回答问题：\n",
    "        \n",
    "        {context}\n",
    "        \n",
    "        问题：{query}\n",
    "        \"\"\"\n",
    "        \n",
    "        # 4. 生成回答\n",
    "        return self.llm.predict(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 高级配置\n",
    "1.链类型选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同链类型的配置\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # 直接将所有文档合并\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "qa_map_reduce = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",  # 分步处理大量文档\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "qa_refine = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",  # 逐步细化答案\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.检索器配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义检索器配置\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # 最大边际相关性搜索\n",
    "    search_kwargs={\n",
    "        \"k\": 3,  # 返回文档数量\n",
    "        \"fetch_k\": 10,  # 预筛选数量\n",
    "        \"lambda_mult\": 0.7  # 多样性权重\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 实际应用示例\n",
    "1. 知识库问答系统\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeBaseQA:\n",
    "    def __init__(self, documents):\n",
    "        # 初始化嵌入模型\n",
    "        self.embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "        \n",
    "        # 创建向量存储\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # 初始化LLM\n",
    "        self.llm = Ollama(model=\"llama2\")\n",
    "        \n",
    "        # 创建QA链\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever(),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "    \n",
    "    def ask(self, question):\n",
    "        result = self.qa_chain.invoke({\"query\": question})\n",
    "        return {\n",
    "            \"answer\": result[\"result\"],\n",
    "            \"sources\": [doc.page_content for doc in result[\"source_documents\"]]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 多语言问答系统\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualQA:\n",
    "    def __init__(self):\n",
    "        self.qa_chains = {}\n",
    "        \n",
    "    def add_language(self, lang, documents):\n",
    "        # 为每种语言创建独立的QA链\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=OllamaEmbeddings(model=\"bge-m3\")\n",
    "        )\n",
    "        \n",
    "        self.qa_chains[lang] = RetrievalQA.from_chain_type(\n",
    "            llm=Ollama(model=\"llama2\"),\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=vectorstore.as_retriever()\n",
    "        )\n",
    "    \n",
    "    def ask(self, question, lang):\n",
    "        if lang not in self.qa_chains:\n",
    "            raise ValueError(f\"Language {lang} not supported\")\n",
    "        \n",
    "        return self.qa_chains[lang].invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 性能优化\n",
    "1. 缓存机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "class CachedQA:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "    \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def get_answer(self, question):\n",
    "        return self.qa_chain.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 批处理优化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchQA:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "    \n",
    "    async def process_batch(self, questions, batch_size=5):\n",
    "        results = []\n",
    "        for i in range(0, len(questions), batch_size):\n",
    "            batch = questions[i:i + batch_size]\n",
    "            tasks = [\n",
    "                self.qa_chain.ainvoke({\"query\": q})\n",
    "                for q in batch\n",
    "            ]\n",
    "            batch_results = await asyncio.gather(*tasks)\n",
    "            results.extend(batch_results)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 最佳实践\n",
    "1. 错误处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustQA:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "    \n",
    "    def safe_ask(self, question):\n",
    "        try:\n",
    "            result = self.qa_chain.invoke({\"query\": question})\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"answer\": result[\"result\"],\n",
    "                \"sources\": result.get(\"source_documents\", [])\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": str(e),\n",
    "                \"fallback_answer\": \"抱歉，我现在无法回答这个问题。\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 结果验证\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_answer(result):\n",
    "    # 检查答案质量\n",
    "    answer = result[\"result\"]\n",
    "    sources = result[\"source_documents\"]\n",
    "    \n",
    "    # 验证答案长度\n",
    "    if len(answer) < 10:\n",
    "        return False, \"答案过短\"\n",
    "    \n",
    "    # 验证源文档支持\n",
    "    if not sources:\n",
    "        return False, \"没有找到相关文档支持\"\n",
    "    \n",
    "    return True, \"验证通过\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetrievalQA是构建智能问答系统的强大工具，它能够：\n",
    "- 结合文档检索和LLM能力\n",
    "- 提供基于上下文的准确回答\n",
    "- 支持多种处理策略\n",
    "- 适应不同规模的应用需求\n",
    "  \n",
    "通过合理配置和优化，RetrievalQA可以帮助我们构建高质量的问答系统。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
