{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent DataFrame Question-Answering System Design Based on RAG Architecture\n",
    "\n",
    "This system implements an intelligent data analysis and query system based on RAG (Retrieval Augmented Generation), allowing interaction with DataFrames using natural language.\n",
    "\n",
    "**Contents**\n",
    "\n",
    "<img src=\"Contents.png\" alt=\"contents\" width=\"300\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Overview\n",
    "\n",
    "### 1.1. System Components Overview\n",
    "\n",
    "1. **Data Documentation (create_df_documents)**\n",
    "- Convert DataFrame information into a searchable document collection, including metadata, contextual information, and information about similar datasets.\n",
    "\n",
    "2. **Vector Store Setup (setup_vectorstore)**\n",
    "- Create and populate a vector database, storing document embeddings to support semantic search.\n",
    "  \n",
    "3. **Question-Answering Chain Creation (create_qa_chain)**\n",
    "- Retrieve relevant context from the vector store, format prompts, and invoke the language model. Parse the results and return a structured response.\n",
    "\n",
    "4. **Query Execution (query_dataframe)**\n",
    "- Accept user queries and call the question-answering chain. Process the queries (pandas code execution) and return the results.\n",
    "  \n",
    "5. **Result Display (display_query_result)**\n",
    "- Format and display query results, showing the original query and the analytical results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. System Workflow\n",
    "\n",
    "The user submits a query in natural language (**User Input**). The query processing system (**query_dataframe**) receives the query and calls the question-answering chain (**create_qa_chain**) for processing. The question-answering chain (**create_qa_chain**) retrieves relevant information from the vector store (**setup_vectorstore**), invokes the LLM to generate an answer, and returns the result to the query processing system (**query_dataframe**). The query processing system (**query_dataframe**) processes the answer (executes pandas code based on the generated code) and sends the result to the result display system (**display_query_result**). Finally, the result display system (**display_query_result**) formats the output and presents it to the user for easy viewing.\n",
    "\n",
    "The diagram below illustrates the complete workflow of the RAG system's question-answering process.\n",
    "\n",
    "<img src=\"workflow.png\" alt=\"Workflow\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Components Design \n",
    "\n",
    "This RAG system is built using the LangChain framework, an open-source framework designed for building and deploying applications powered by language models. LangChain provides a suite of tools and components for processing and interacting with language models, making it suitable for a variety of natural language processing tasks such as question answering, conversational systems, text generation, and more.\n",
    "\n",
    "LangChain offers a modular framework, enabling independent development and testing of different components (e.g., document retrieval, LLM calls, output parsing). This modular design simplifies system maintenance and enhances scalability. Additionally, LangChain allows users to choose from various LLMs, embedding models, and retrievers based on their specific needs. This flexibility enables developers to select the most suitable components for their use cases and datasets, thereby improving system performance. \n",
    "\n",
    "Furthermore, LangChain provides a simplified API that enables developers to rapidly build complex workflows. Through chain-based invocation, developers can seamlessly connect multiple processing steps, creating a comprehensive query handling pipeline.\n",
    "\n",
    "### 2.1. Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.embeddings import Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported packages overview:\n",
    "\n",
    "- `pd`: DataFrame operations and data analysis\n",
    "- `List`: Python type annotation for lists\n",
    "- `Optional`: Python type annotation for optional values and can be None\n",
    "- `Chroma`: Vector database for storing and searching embeddings\n",
    "- `Document`: Container for text content and metadata\n",
    "- `OllamaLLM`: Interface for local LLM models\n",
    "- `PromptTemplate`: Template builder for LLM inputs\n",
    "- `StrOutputParser`: Converts LLM outputs to strings\n",
    "- `HuggingFaceEmbeddings`: Text embedding models from HuggingFace\n",
    "- `Embeddings` : Base class for all embedding models that convert text to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Create Data Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, design a create_df_documents function. The function converts the basic informations of a dataframe into a structured list of document collections for subsequent retrieval by the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_documents(source: str, creation_date: str, last_updated: str, purpose: str, content_description: str, similar_datasets: List[str]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Convert DataFrame information into a list of documents for RAG system retrieval.\n",
    "    \n",
    "    This function creates structured documents containing:\n",
    "    - Metadata information (e.g., source, creation date, last updated)\n",
    "    - Contextual information that can be customized by the user\n",
    "    - Similar datasets information that provide some datasets similar to this dataset\n",
    "    \n",
    "    Args:\n",
    "        source (str): Source of the data\n",
    "        creation_date (str): Creation date of the DataFrame\n",
    "        last_updated (str): Last updated date of the DataFrame\n",
    "        purpose (str): Purpose of the DataFrame\n",
    "        content_description (str): Brief description of the content of the DataFrame\n",
    "        similar_datasets (List[str]): List of similar datasets\n",
    "        \n",
    "    Returns:\n",
    "        List[Document]: List of Document objects, each containing:\n",
    "            - page_content: String containing specific DataFrame information\n",
    "            - metadata: Dictionary with type information ('metadata_info', 'context_info', 'similar_datasets_info')\n",
    "    \n",
    "    Example:\n",
    "        >>> df = pd.DataFrame({'product_id': [1, 2], 'price': [10.99, 15.99]})\n",
    "        >>> documents = create_df_documents(\n",
    "        ...     source=\"Kaggle Amazon Products Dataset\",\n",
    "        ...     creation_date=\"2023-01-01\",\n",
    "        ...     last_updated=\"2023-10-01\",\n",
    "        ...     purpose=\"Analyze product sales trends\",\n",
    "        ...     content_description=\"Product details including price.\",\n",
    "        ...     similar_datasets=[\"Kaggle Amazon Products Dataset\", \"Kaggle Amazon Reviews Dataset\"]\n",
    "        ... )\n",
    "        >>> print(documents[0].page_content)  # Metadata information\n",
    "        >>> print(documents[1].page_content)  # Contextual information\n",
    "    \"\"\"\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    # Metadata information\n",
    "    metadata_info = (f\"Data Source Info: {source}, \"\n",
    "                     f\"Creation Date: {creation_date}, \"\n",
    "                     f\"Last Updated: {last_updated}\")\n",
    "    documents.append(Document(\n",
    "        page_content=metadata_info,\n",
    "        metadata={\"type\": \"metadata_info\"}\n",
    "    ))\n",
    "    \n",
    "    # Contextual information\n",
    "    context_info = (f\"This DataFrame is intended for: {purpose}. \"\n",
    "                    f\"It contains data related to: {content_description}.\")\n",
    "    documents.append(Document(\n",
    "        page_content=context_info,\n",
    "        metadata={\"type\": \"context_info\"}\n",
    "    ))\n",
    "    \n",
    "    # Similar datasets information\n",
    "    similar_datasets_info = \"Similar datasets:\\n\" + \"\\n\".join(f\"- {dataset}\" for dataset in similar_datasets)\n",
    "    documents.append(Document(\n",
    "        page_content=similar_datasets_info,\n",
    "        metadata={\"type\": \"similar_datasets_info\"}\n",
    "    ))\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, I have included some information that cannot be directly inferred from the DataFrame itself, such as the data source, creation date, last updated date, as well as a description of the purpose and content of the DataFrame.\n",
    "\n",
    "Then, use `Document` objects to store the information, where each document contains both page_content and metadata. The page_content stores the data information, while the metadata stores the type information.\n",
    "\n",
    "This approach provides large language models with more comprehensive contextual information, improving the accuracy of responses to user queries. Additionally, by leveraging a structured storage design, the model can first index the metadata types and then perform targeted searches, thereby enhancing search efficiency.\n",
    "\n",
    "Additionally, by enabling the function to accept additional parameters, users can freely record and describe relevant information for different DataFrames, making the function adaptable to various scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Setup Vector Store\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I designed a setup_vectorstore function. The function creates a vector database to store and retrieve document embeddings efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vectorstore(documents: List[Document], embedding: Optional[Embeddings] = None) -> Chroma:\n",
    "    \"\"\"\n",
    "    Create and populate a vector store for efficient document retrieval.\n",
    "    \n",
    "    This function initializes a Chroma vector database with document embeddings for \n",
    "    semantic search capabilities. It either uses a provided embedding model or defaults \n",
    "    to HuggingFace's all-MiniLM-L6-v2 model.\n",
    "    \n",
    "    Args:\n",
    "        documents (List[Document]): List of Document objects to be stored in the vector database\n",
    "        embedding (Optional[Embeddings]): Embedding model to convert text to vectors. \n",
    "            Defaults to HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "    Returns:\n",
    "        Chroma: Initialized vector store containing document embeddings for similarity search\n",
    "    \n",
    "    Example:\n",
    "        >>> documents = create_df_documents(df)\n",
    "        >>> vectorstore = setup_vectorstore(documents)\n",
    "        >>> # Or with custom embedding model:\n",
    "        >>> vectorstore = setup_vectorstore(documents, custom_embedding_model)\n",
    "    \"\"\"\n",
    "   \n",
    "    if embedding is None:\n",
    "        embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding,\n",
    "        collection_name=\"df_rag\"\n",
    "    )\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function converts the structured document list generated in the create_df_documents function into embeddings using an embedding model `all-MiniLM-L6-v2`. `Chroma` then stores these embeddings for semantic search, which is critical for context retrieval in the RAG system.\n",
    "\n",
    "\n",
    "The all-MiniLM-L6-v2 model is used defualt because it performs well on product description texts, processes quickly, is suitable for large datasets, has low resource consumption, strong community support, and high stability.\n",
    "\n",
    "I have also designed an Optional[Embeddings] parameter here, allowing the use of other embedding models in addition to the default one. This enhances the function's scalability.\n",
    "\n",
    "Chroma is chosen for vectorized storage because it is lightweight, supports local storage, performs well, is easy to use, and is suitable for handling large-scale data with efficient similarity search capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Create Question-Answering Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a create_qa_chain function is designed. This function implements an intelligent system capable of understanding natural language questions and generating accurate, structured DataFrame analysis results. Using the RAG (Retrieval-Augmented Generation) architecture, it provides more precise answers based on actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain(vectorstore: Chroma, llm, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create an intelligent question-answering chain for DataFrame analysis using RAG architecture.\n",
    "    \n",
    "    This function builds a chain that:\n",
    "    - Retrieves relevant context from the vector store\n",
    "    - Formats prompts with specific instructions\n",
    "    - Processes queries through LLM\n",
    "    - Returns structured responses with data analysis results\n",
    "    \n",
    "    Args:\n",
    "        vectorstore (Chroma): Vector database containing DataFrame documentation\n",
    "        llm: Language model for generating responses\n",
    "        df (pd.DataFrame): DataFrame being analyzed\n",
    "        \n",
    "    Returns:\n",
    "        Chain: A callable chain that takes a question string and returns:\n",
    "            - Data Source: Description of data used\n",
    "            - Method: Description of analysis approach\n",
    "            - Code: Optional pandas code block\n",
    "            - Result: Analysis results\n",
    "    \n",
    "    Example:\n",
    "        >>> qa_chain = create_qa_chain(vectorstore, llm, df)\n",
    "        >>> result = qa_chain.invoke(\"What is the average price?\")\n",
    "        >>> print(result)\n",
    "    \"\"\"\n",
    "    \n",
    "    template = \"\"\"You are a dataframe analysis assistant. Provide concise answers with only 4 sections:\n",
    "                1. RAG Data Source: [One line description of data used]\n",
    "                2. Method: [One line description of analysis approach]\n",
    "                3. Code: [If code is needed, write one line of pandas code in ```python``` block, \n",
    "                          and copy the execution result to the variable `result`.\n",
    "                          EXAMPLE:code:\\n\n",
    "                                  ```python\\n\n",
    "                                     result = df['category_name'].value_counts().count()\\n\n",
    "                                ```\\n\n",
    "                          ]\n",
    "                4. Result: [Concise results only]\n",
    "\n",
    "                Context:{context}\n",
    "                Question: {question}\n",
    "                \n",
    "                Note: Use the existing DataFrame 'df' provided by the system, do not create or read a new one.\n",
    "                \n",
    "                You can use these pandas operations:\n",
    "                1. Basic statistics: df.describe(), df[column].mean(), df[column].max(), etc.\n",
    "                2. Group statistics: df.groupby(column).agg()\n",
    "                3. Sorting: df.sort_values(by=column)\n",
    "                4. Filtering: df[df[column] > value]\n",
    "                ...\n",
    "\n",
    "                Rules:\n",
    "                1. Keep each section to ONE line only.\n",
    "                2. No explanations or additional text.\n",
    "                3. If code is needed, write complete executable code in ```python``` block using the existing 'df'.\n",
    "                4. Always assign the final result to a variable named 'result'.\n",
    "                5. Use proper column names from the DataFrame.\n",
    "                \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 3}\n",
    "    )\n",
    "    \n",
    "    def format_chain_input(question):\n",
    "        return {\n",
    "            \"context\": retriever.invoke(question),\n",
    "            \"question\": question\n",
    "        }\n",
    "    \n",
    "    chain = (\n",
    "        format_chain_input\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, I first designed a prompt template that defines the structure of the response, includes context information retrieved by the vector retriever, the user’s question, specific tips for commonly asked Python operations, and clear rules. This approach forces the LLM to generate and output responses in a specified direction, making the system’s answers more standardized and stable. (Through multiple tests with different prompt templates, I found that the content of the prompt significantly impacts the system's final output. Detailed and clear instructions greatly improve the accuracy and stability of the results.) The text template is then converted into a LangChain `PromptTemplate` object for structured input management, with two input variables defined: context and question, to prevent omissions or incorrect inputs.\n",
    "\n",
    "Using the vector database created in the `setup_vectorstore` function, I converted the database into a retriever object by calling the `as_retriever` method. This retriever uses similarity search to return the 3 most relevant documents for each query. This step is the core of the RAG architecture. With similarity searching via the vector database, it quickly retrieves the documents most relevant to the user's query. I chose the top 3 relevant documents to provide sufficient context while avoiding information overload, ensuring that the LLM receives accurate context information and improving the relevance and accuracy of the generated results.\n",
    "\n",
    "The `format_chain_input` function converts the relevant context retrieved by the retriever and the user's question into a standardized dictionary format, aligning with the input variable requirements of the PromptTemplate. This facilitates data transmission in subsequent chain processing.\n",
    "\n",
    "This design ensures that each component receives properly formatted input, making data flow through the processing chain clearer and more controllable.\n",
    "\n",
    "Finally, a complete question-answering chain is constructed using the pipeline operator `|`, which executes sequentially: format input -> apply the prompt template -> call the LLM model -> parse the output into a string. This chain-based design, with each component being independent and replaceable, makes the system easy to debug and modify. Components can be flexibly added, removed, or replaced to meet different requirements and adapt to changes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Query Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, a query_dataframe function is created.This function is an \"intelligent query executor\" that converts natural language queries into actual data analysis results and returns them in a standardized format. It serves as a critical bridge connecting user queries, LLM understanding, and real-world data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_dataframe(question: str, qa_chain, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Execute DataFrame queries and process LLM responses with code execution capabilities.\n",
    "    \n",
    "    This function processes natural language queries through the QA chain and handles\n",
    "    code execution when necessary. It:\n",
    "    - Gets response from the QA chain\n",
    "    - Detects if response contains executable code\n",
    "    - Executes code if present and updates results\n",
    "    - Handles errors in both query processing and code execution\n",
    "    \n",
    "    Args:\n",
    "        question (str): Natural language query about the DataFrame\n",
    "        qa_chain: Question-answering chain created by create_qa_chain\n",
    "        df (pd.DataFrame): DataFrame to be queried\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted response containing:\n",
    "            - Original LLM response if no code execution needed\n",
    "            - Updated response with actual execution results if code present\n",
    "            - Error message if execution fails\n",
    "    \n",
    "    Example:\n",
    "        >>> result = query_dataframe(\"What is the average price?\", qa_chain, df)\n",
    "        >>> print(result)\n",
    "        Data Source: Price column from DataFrame\n",
    "        Method: Calculate mean price\n",
    "        Code: ```python\n",
    "        result = df['price'].mean()\n",
    "        ```\n",
    "        Result: 25.99\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get response from LLM\n",
    "        answer = qa_chain.invoke(question)\n",
    "        \n",
    "        # If no code block exists, return the original answer\n",
    "        if \"```python\" not in answer:\n",
    "            return answer\n",
    "            \n",
    "        # Extract code block\n",
    "        code_start = answer.find(\"```python\") + 9\n",
    "        code_end = answer.find(\"```\", code_start)\n",
    "        code = answer[code_start:code_end].strip()\n",
    "        \n",
    "        try:\n",
    "            # Create local namespace and execute code\n",
    "            local_dict = {'df': df, 'pd': pd}\n",
    "            if 'result =' not in code:\n",
    "                code = f\"result = {code}\"\n",
    "            exec(code, None, local_dict)\n",
    "            result = local_dict.get('result')\n",
    "            \n",
    "            # Update Result section\n",
    "            result_section_start = answer.find(\"Result:\")\n",
    "            if result_section_start != -1:\n",
    "                next_section = answer.find(\"\\n\", result_section_start)\n",
    "                if next_section == -1:\n",
    "                    next_section = len(answer)\n",
    "                \n",
    "                answer = (\n",
    "                    answer[:result_section_start + 7] +  # Include \"Result: \"\n",
    "                    \"\\n\" + str(result) +                 # Add execution result\n",
    "                    answer[next_section:]                # Add remaining content\n",
    "                )\n",
    "            \n",
    "            return answer\n",
    "         \n",
    "        except SyntaxError as syntax_error:\n",
    "            print(f\"Code execution error: Invalid syntax in code: {code}\\nError: {str(syntax_error)}\")\n",
    "            return answer   \n",
    "        except Exception as code_error:\n",
    "            print(f\"Code execution error: {str(code_error)}\")\n",
    "            return answer\n",
    "            \n",
    "    except Exception as e:  \n",
    "        return {'result': None, 'error': f\"Query error: {str(e)}\"}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a try-except block at the outermost layer allows the function to capture all potential errors, ensuring the function does not crash when encountering issues but instead returns meaningful error messages.\n",
    "\n",
    "First, the `qa_chain` is called to process the query and obtain the LLM's response. (Based on the previously defined prompt template, we specified \"If code is needed, write one line of pandas code in ```python``` block.\" Therefore, if Python operations are required, the response will include a code block.)\n",
    "\n",
    "The response is then checked for the presence of a Python code block. If none is found, it indicates a pure text response, and the LLM's answer is returned directly. This approach avoids unnecessary code execution.\n",
    "\n",
    "If a code block is present, its content is extracted, a local namespace is created, and the code is executed. The execution result is then updated in the \"Result\" section of the response, and the updated response is returned.\n",
    "\n",
    "This logical architecture design enhances the system's flexibility in answering queries, allowing it to handle both pure text responses and those requiring code execution seamlessly. Executing code in an isolated namespace avoids polluting the global environment, enhancing system security. Furthermore, by only updating the actual result section, the original structure of the response is preserved, ensuring consistent response formatting and improving the user experience.If the code execution encounters an error, the system will still return the RAG system's answer, enhancing the robustness of the system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Result Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design the display_query_result function to format and display query results.This function acts as a \"presentation steward,\" responsible for delivering query results to users in an elegant, clear, and professional manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_query_result(question, qa_chain, df):\n",
    "    \"\"\"\n",
    "    Format and display query results in a structured and visually appealing way.\n",
    "    \n",
    "    This function handles the presentation of query results, including:\n",
    "    - Displaying the original query\n",
    "    - Formatting the response with clear section breaks\n",
    "    - Visual separators for better readability\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's natural language query\n",
    "        qa_chain: Question-answering chain created by create_qa_chain\n",
    "        df (pd.DataFrame): DataFrame being queried\n",
    "        \n",
    "    Prints:\n",
    "        - Query header with separators\n",
    "        - Formatted answer\n",
    "        \n",
    "    Example:\n",
    "        >>> display_query_result(\"What is the average price?\", qa_chain, df)\n",
    "        =================================================\n",
    "        📝 Query: What is the average price?\n",
    "        =================================================\n",
    "        \n",
    "        📊 Answer:\n",
    "        ------------------------------\n",
    "        Data Source: Price column from DataFrame\n",
    "        Method: Calculate mean price\n",
    "        Code: ```python\n",
    "        result = df['price'].mean()\n",
    "        ```\n",
    "        Result: 25.99\n",
    "        =================================================\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"📝 Query: {question}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    result = query_dataframe(question, qa_chain, df)\n",
    "    \n",
    "    print(\"\\n📊 Answer:\")\n",
    "    print(\"-\"*30)\n",
    "    print(f\"{result}\")\n",
    "    print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By incorporating dividers and icons, clear visual boundaries were created. This design makes the query results easier to read and understand, ensuring that serving as the interface between users and the query system, users can effortlessly interpret and utilize the query results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Implementation and Verification\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Test Environment Setup\n",
    "#### 3.1.1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The dataset used for this test is from the Kaggle platform, and the download link is:https://www.kaggle.com/datasets/asaniczka/amazon-products-dataset-2023-1-4m-products/. \n",
    "\n",
    "This dataset includes two CSV files: amazon_products.csv and amazon_categories.csv.The amazon_products.csv and amazon_categories.csv are linked through a foreign key relationship where the 'category_id' column in the 'amazon_products' references the 'id' column in the 'amazon_categories', allowing us to connect each product to its corresponding category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `load_amazon_data` function, I read the two CSV files into a single DataFrame and performed some data cleaning and preprocessing operations. Below, I will directly call this function to load our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/zhangjing/Desktop/tfm\")\n",
    "from version2 import load_amazon_data\n",
    "\n",
    "df = load_amazon_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1426337, 12)\n",
      "\n",
      "Columns: ['asin', 'title', 'imgUrl', 'productURL', 'stars', 'reviews', 'price', 'listPrice', 'category_id', 'isBestSeller', 'boughtInLastMonth', 'category_name']\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1426337 entries, 0 to 1426336\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count    Dtype  \n",
      "---  ------             --------------    -----  \n",
      " 0   asin               1426337 non-null  object \n",
      " 1   title              1426336 non-null  object \n",
      " 2   imgUrl             1426337 non-null  object \n",
      " 3   productURL         1426337 non-null  object \n",
      " 4   stars              1426337 non-null  float64\n",
      " 5   reviews            1426337 non-null  int64  \n",
      " 6   price              1426337 non-null  float64\n",
      " 7   listPrice          1426337 non-null  float64\n",
      " 8   category_id        1426337 non-null  int64  \n",
      " 9   isBestSeller       1426337 non-null  bool   \n",
      " 10  boughtInLastMonth  1426337 non-null  int64  \n",
      " 11  category_name      1426337 non-null  object \n",
      "dtypes: bool(1), float64(3), int64(3), object(5)\n",
      "memory usage: 121.1+ MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nInfo:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 1,426,337 rows and 12 columns. Below is a brief description of each column:\n",
    "\n",
    "- asin: Product ID from Amazon. (type:object)\n",
    "- title: Title of the product. (type:object)\n",
    "- imgUrl: Url of the product image. (type:object)\n",
    "- productURL: Url of the product. (type:object)\n",
    "- stars: Product rating. If 0, no ratings were found. (type:float64)\n",
    "- reviews: Number of reviews. If 0, no reviews were found. (type:int64)\n",
    "- price: Buy now price of the product. If 0, price was unavailable. (type:float64, currency: USD)\n",
    "- listPrice: Original price of the product before discount. If 0, no list price was found AKA, no discounts. (type:float64, currency: USD)\n",
    "- category_id: Use the amazon_categories.csv to find the actual category name. (type:int64)\n",
    "- isBestSeller: Whether the product had the Amazon BestSeller status or not. (type:bool)\n",
    "- boughtInLastMonth: Number of times the product was bought in the last month. (type:int64)\n",
    "- category_name: Name of the category as on Amazon.com. (type:object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Called Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, use the create_df_documents function, which converts the basic information of the DataFrame into a structured list of document collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_datasets = [\n",
    "    \"Amazon UK Products\",\n",
    "    \"Amazon Canada Products\",\n",
    "    \"Amazon India Products\"\n",
    "]\n",
    "\n",
    "content_description=\"\"\"Amazon is one of the biggest online retailers in the USA \n",
    "                        that sells over 12 million products. With this dataset, you \n",
    "                        can get an in-depth idea of what products sell best, which \n",
    "                        SEO titles generate the most sales, the best price range\n",
    "                        for a product in a given category, and much more.\"\"\"\n",
    "\n",
    "documents = create_df_documents(\n",
    "    source=\"Kaggle Amazon Products Dataset\",\n",
    "    creation_date=\"2023-01-01\",\n",
    "    last_updated=\"2024-01-15\",\n",
    "    purpose=\"query dataset information using natural language based on the RAG architecture\",\n",
    "    content_description=content_description,\n",
    "    similar_datasets=similar_datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, pass the structured list of document collections generated in the previous step into the setup_vectorstore function. For this test, the function's default embedding model, all-MiniLM-L6-v2, is used. As mentioned earlier during the design of the setup_vectorstore function, this embedding model performs well on descriptive datasets like this one. The resulting vectorstore database is then assigned to the vectorstore variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = setup_vectorstore(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the create_qa_chain function is called with the DataFrame df, the vectorstore database vectorstore, and the LLM as parameters to generate the QA chain.\n",
    "\n",
    "For this test, the LLM selected is the locally running Llama 3.1 model. This model is fully open-source, easy to deploy and use via Ollama, and can run entirely offline without requiring an internet connection. It offers fast response times and excellent support for structured data analysis, enabling the generation of high-quality Pandas code.\n",
    "\n",
    "At the same time, the temperature is set to 0.75, which is a balanced value—neither too conservative (temperature close to 0) nor too random (temperature close to 1). This ensures a balance between maintaining accuracy in responses and allowing a degree of creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1\", temperature=0.75)\n",
    "qa_chain = create_qa_chain(vectorstore, llm, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. System Testing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Test Design Approach\n",
    "\n",
    "##### 3.2.1.1. Test Objectives\n",
    "\n",
    "The primary objectives of this test are to evaluate the system in the following four aspects:\n",
    "\n",
    "- **Functionality**: Verify if the system can accurately interpret natural language queries and perform the corresponding DataFrame operations.\n",
    "- **Accuracy**: Ensure that the generated Pandas code aligns with the query intent and returns correct results.\n",
    "- **Robustness**: Assess the system's performance under boundary conditions and abnormal inputs.\n",
    "- **Readability**: Evaluate whether the output format of the responses is clearly structured and easy for users to understand.\n",
    "\n",
    "##### 3.2.1.2. Test Dimension Categorization\n",
    "\n",
    "1. **Functional Test Dimensions** (to evaluate system functionality and accuracy):\n",
    "\n",
    "- **Basic Operations**: Querying basic information such as the number of rows, columns, column names, etc.\n",
    "- **Data Quality Checks**: Querying for missing values or duplicate data.\n",
    "- **Filtering and Grouping Operations**: Queries that involve conditional filtering or grouping and aggregation.\n",
    "- **Inter-column Relationships**: Queries related to correlation information between numerical columns.\n",
    "- **Statistical Analysis**: Queries for summary statistics, maximum, minimum, median values of numerical columns, etc.\n",
    "\n",
    "2. **Robustness Test Dimensions** (to evaluate system robustness and readability):\n",
    "\n",
    "- **Clarity of Natural Language Input**: Compare the system's responses to well-defined and ambiguous queries.\n",
    "\n",
    "  - *Well-defined queries*: Precisely specify column names in the DataFrame, allowing answers to be directly derived using queries or Pandas operations. Questions use standard statistical terminology.\n",
    "  \n",
    "  - *Ambiguous queries*: Do not specify exact column names in the DataFrame, use synonyms or abbreviations, and require the system to infer answers by making comprehensive judgments. Questions use vague statistical terminology.\n",
    "\n",
    "- **Complexity of Natural Language Input**: Compare the system's responses to simple and complex queries.\n",
    "\n",
    "  - *Simple queries*: Involve single columns, with answers being a single value.\n",
    "  \n",
    "  - *Complex queries*: Combine multiple columns for calculations, with answers potentially involving multiple rows of data.\n",
    "\n",
    "- **Handling of Abnormal Input**: Evaluate the system's behavior when users ask unrelated questions or make spelling mistakes in the query.\n",
    "\n",
    "##### 3.2.1.3. Test Criteria\n",
    "\n",
    "- **Functionality Pass Rate**: At least 90% of the test cases must pass.\n",
    "- **Accuracy Requirements**: Query results must match manually computed results.\n",
    "- **Robustness**:\n",
    "  - a. The system must not crash under abnormal inputs and should provide clear error messages.\n",
    "  - b. When questions are ambiguous, involve complex operations, or contain minor spelling errors, the system should handle them correctly and return appropriate results.\n",
    "- **Readability**: The output format should maintain consistent structure, logical layout, and clearly display the query results.\n",
    "\n",
    "#### 3.2.2. Test Execution\n",
    "\n",
    "##### 3.2.2.1. Test Case Design\n",
    "\n",
    "1. **Functional Test Cases**\n",
    "\n",
    "| Category                 | Test Case              | Input Question                                          | Expected Result                                                                                              | Validation Method                                                                                      |\n",
    "|--------------------------|---------------------------|--------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|\n",
    "| **Basic Information Test** | Query number of rows and columns | How many rows and columns?                             | (1426337, 12)                                                                                               | Compare the return value of `result` with `df.shape`.                                                |\n",
    "|                          | Query column names        | What are the column names?                             | ['asin', 'title', 'imgURL', 'productURL', 'stars', 'reviews', 'price', 'listPrice', 'category_id', 'isBestSeller', 'boughtInLastMonth', 'category_name'] | Compare the return value of `result` with `list(df.columns)`.                                       |\n",
    "| **Data Quality Test**     | Missing value statistics  | How many missing values are there in the dataset?      | 1                                                                                                            | Compare the return value of `result` with `df.isnull().sum().sum()`.                                |\n",
    "|                          | Duplicate value statistics | How many duplicate values?                             | 0                                                                                                            | Compare the return value of `result` with `df.duplicated().sum()`.                                  |\n",
    "| **Column Relationship Test** | Query correlation between two numeric columns | What's the correlation between the reviews and isBestSeller columns? | 0.0940852770174714                                                                                         | Compare the return value of `result` with `df['reviews'].corr(df['isBestSeller'])`.                 |\n",
    "| **Filtering and Grouping** | Query average price by category name | Average price by category name?                        | Statistics containing two columns; content not fully written here.                                          | Compare the return value of `result` with `df.groupby('category_name')['price'].mean()`.            |\n",
    "|                          | Query BestSeller data with price > 1000 | Show me all the data where isBestSeller is true and price is more than 1000 | Returns rows of data; content not fully written here.                                                        | Compare the return value of `result` with `df[(df['isBestSeller']==True) & (df['price']>1000)]`.    |\n",
    "| **Statistical Analysis**  | Query overall average price | What is the average price?                              | 43.375430688097                                                                                             | Compare the return value of `result` with `df['price'].mean()`.                                     |\n",
    "|                          | Query unique count in category_name column | How many unique in the category name column?           | 248                                                                                                          | Compare the return value of `result` with `df['category_name'].nunique()`.                          |\n",
    "\n",
    "\n",
    "2. **Robustness Test Cases**\n",
    "\n",
    "| Category             | Test Case                                     | Input Question                                             | Expected Result                                              | Verification Method                    |\n",
    "|----------------------|---------------------------------------------------|------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------|\n",
    "| **Question Clarity Test** | Querying the highest `stars` - Clear             | What's the highest stars in the dataset?                   | 5                                                           | Directly observe the output            |\n",
    "|                      | Best sellers more than $1000 - Fuzzy              | Best sellers more than $1000?                              | Returns 2 rows of data; content not detailed here            | Directly observe the output            |\n",
    "|                      | Information about the DataFrame - Fuzzy           | Tell me about the DataFrame.                               | Output the content in the document or the summary of the DataFrame.       | Directly observe the output            |\n",
    "| **Question Complexity Test** | Querying the average price - Simple             | What is the average price?                                 | 43.375403680897                                              | Directly observe the output            |\n",
    "|                      | Querying DataFrame creation date - Simple         | When was this dataframe created?                           | \"2023-01-01\"                                                | Directly observe the output            |\n",
    "|                      | Querying the top 5 titles with the highest stars and lowest price - Complex | Show me the top 5 titles with the highest stars and lowest price | ['Capri 2.0 27-Inch Spinner --', 'DSP Men\\'s Performance Stretch Pants', 'DSP Lightweight Rain Shell Jacket', 'DSP Men\\'s High Heat Polo - Short Sleeve', 'DSP Men\\'s High Heat Polo - Long Sleeve'] | Directly observe the output            |\n",
    "| **Error Test**       | Typographical error test                          | Hwo mayn culomns?                                          | Correct output, 12                                          | Directly observe the output            |\n",
    "|                      | Irrelevant question                               | What is the weather today?                                 | Executes correctly, output says it doesn't know             | Directly observe the output            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2.2. Test Case Execution\n",
    "\n",
    "\n",
    "I use Python's `unittest` framework to execute test cases. This approach has many advantages, such as automating the execution of test cases, saving time and effort compared to manual testing, and allowing test cases to be organized into classes and methods, making the test code easier to manage and maintain.\n",
    "\n",
    "Before developing the test case system, I first wrote a general test case execution function, `run_specific_test`, to run specific test cases in the testing system and output the results. This approach allows for the flexible development of various test systems and test cases while providing a unified interface for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import unittest\n",
    "\n",
    "def run_specific_test(test_name, test_class):\n",
    "    \"\"\"Run a specific test from the test class\n",
    "    \n",
    "    Args:\n",
    "        test_name (str): Name of the test method to run\n",
    "        test_class (class): Test class\n",
    "        \n",
    "    Example:\n",
    "        # Run functional test\n",
    "        run_specific_test('test_basic_queries', TestRAGQueryFunctional)\n",
    "    \"\"\"\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(test_class(test_name))\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    runner.run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Functional Testing**\n",
    "\n",
    "In this section, test scripts are written based on the previously designed functional test cases. All test cases are executed sequentially, and the system outputs are compared with the expected results.\n",
    "\n",
    "Below, I have developed a test case class, `TestRAGSystemFunctional`, based on Python's `unittest` framework (`unittest.TestCase`), to systematically perform unit testing on the functionality of the RAG (Retrieval-Augmented Generation) system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestRAGSystemFunctional(unittest.TestCase):\n",
    "    \n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"Initialize the test environment\"\"\"\n",
    "        cls.qa_chain = qa_chain\n",
    "        cls.df = df\n",
    "        cls.test_results = []\n",
    "        \n",
    "    def run_test_cases(self, test_cases, category):\n",
    "        print(f\"\\n=== Testing {category} ===\\n\")\n",
    "        \n",
    "        results = []\n",
    "        for case in test_cases:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result = {\n",
    "                'category': category,\n",
    "                'query': case['query'],\n",
    "                'rag_result': query_dataframe(case['query'], self.qa_chain, self.df),\n",
    "                'expected_result': eval(case['pandas_code']),\n",
    "                'execution_time': time.time() - start_time\n",
    "            }\n",
    "            \n",
    "            self._print_test_result(result)\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        return results\n",
    "            \n",
    "    def _print_test_result(self, result):\n",
    "        print(f\"📝 Query: {result['query']}\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\n📊 RAG Result:\")\n",
    "        print(result['rag_result'])\n",
    "        print(\"\\n✅ Expected Result (Pandas):\")\n",
    "        print(result['expected_result'])\n",
    "        print(f\"Execution Time: {result['execution_time']:.2f}s\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    def test_basic_queries(self):\n",
    "        test_cases = [\n",
    "            {\n",
    "                'query': \"How many rows and columns?\",\n",
    "                'pandas_code': \"df.shape\"\n",
    "            },\n",
    "            {\n",
    "                'query': \"What are the column names?\",\n",
    "                'pandas_code': \"list(df.columns)\"\n",
    "            }\n",
    "        ]\n",
    "        return self.run_test_cases(test_cases, \"Basic Queries\")\n",
    "    \n",
    "    def test_quality_queries(self):\n",
    "        test_cases = [\n",
    "            {\n",
    "                'query': \"How many missing values are there in the dataset?\",\n",
    "                'pandas_code': \"df.isnull().sum().sum()\"\n",
    "            },\n",
    "            {\n",
    "                'query': \"How many duplicate values?\",\n",
    "                'pandas_code': \"df.duplicated().sum()\"\n",
    "            }\n",
    "        ]\n",
    "        return self.run_test_cases(test_cases, \"Quality Queries\")\n",
    "    \n",
    "    def test_statistical_queries(self):\n",
    "        test_cases = [\n",
    "            {\n",
    "                'query': \"What is the average price?\",\n",
    "                'pandas_code': \"df['price'].mean()\"\n",
    "            },\n",
    "            {\n",
    "                'query': \"How many unique in the category_name column?\",\n",
    "                'pandas_code': \"df['category_name'].nunique()\"\n",
    "            }\n",
    "        ]\n",
    "        return self.run_test_cases(test_cases, \"Statistical Queries\")\n",
    "    \n",
    "    def test_correlation_queries(self):   \n",
    "        test_cases = [\n",
    "            {\n",
    "                'query': \"What's the correlation between the reviews and isBestSeller columns?\",\n",
    "                'pandas_code': \"df['reviews'].corr(df['isBestSeller'])\"\n",
    "            }\n",
    "        ]\n",
    "        return self.run_test_cases(test_cases, \"Correlation Queries\")\n",
    "    \n",
    "    def test_complex_queries(self):\n",
    "        test_cases = [\n",
    "            {\n",
    "                'query': \"Average price by category_name\",\n",
    "                'pandas_code': \"df.groupby('category_name')['price'].mean()\"\n",
    "            },\n",
    "            {\n",
    "                'query': \"Show me all the data where isBestSeller is true and price is more than 1000\",\n",
    "                'pandas_code': \"df[(df['isBestSeller']==True) & (df['price']>1000)]\"\n",
    "            }\n",
    "        ]\n",
    "        return self.run_test_cases(test_cases, \"Complex Queries\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_basic_queries (__main__.TestRAGSystemFunctional) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Basic Queries ===\n",
      "\n",
      "📝 Query: How many rows and columns?\n",
      "==================================================\n",
      "\n",
      "📊 RAG Result:\n",
      "1. RAG Data Source: Kaggle Amazon Products Dataset\n",
      "2. Method: Using pandas library to get shape of dataframe\n",
      "3. Code: ```python\n",
      "   result = df.shape\n",
      "```\n",
      "4. Result:\n",
      "(1426337, 12)\n",
      "\n",
      "✅ Expected Result (Pandas):\n",
      "(1426337, 12)\n",
      "Execution Time: 11.31s\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 18.291s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Query: What are the column names?\n",
      "==================================================\n",
      "\n",
      "📊 RAG Result:\n",
      "1. RAG Data Source: Kaggle Amazon Products Dataset\n",
      "2. Method: Using pandas, get column names from DataFrame df\n",
      "3. Code: ```python\n",
      "    result = list(df.columns)\n",
      "```\n",
      "4. Result:\n",
      "['asin', 'title', 'imgUrl', 'productURL', 'stars', 'reviews', 'price', 'listPrice', 'category_id', 'isBestSeller', 'boughtInLastMonth', 'category_name']\n",
      "\n",
      "✅ Expected Result (Pandas):\n",
      "['asin', 'title', 'imgUrl', 'productURL', 'stars', 'reviews', 'price', 'listPrice', 'category_id', 'isBestSeller', 'boughtInLastMonth', 'category_name']\n",
      "Execution Time: 6.98s\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_specific_test('test_basic_queries', TestRAGSystemFunctional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_quality_queries (__main__.TestRAGSystemFunctional) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Quality Queries ===\n",
      "\n",
      "📝 Query: How many missing values are there in the dataset?\n",
      "==================================================\n",
      "\n",
      "📊 RAG Result:\n",
      "1. RAG Data Source: Kaggle Amazon Products Dataset\n",
      "2. Method: Counting missing values in the dataset\n",
      "3. Code: ```python\n",
      "result = df.isnull().sum().sum()\n",
      "```\n",
      "4. Result:\n",
      "1\n",
      "\n",
      "✅ Expected Result (Pandas):\n",
      "1\n",
      "Execution Time: 8.37s\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 18.153s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Query: How many duplicate values?\n",
      "==================================================\n",
      "\n",
      "📊 RAG Result:\n",
      "1. RAG Data Source: Amazon UK Products dataset\n",
      "2. Method: Counting duplicate values in the entire DataFrame\n",
      "3. Code: ```python\n",
      "                  result = df.duplicated().sum()\n",
      "               ```\n",
      "4. Result:\n",
      "0\n",
      "\n",
      "✅ Expected Result (Pandas):\n",
      "0\n",
      "Execution Time: 9.78s\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_specific_test('test_quality_queries', TestRAGSystemFunctional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_statistical_queries (__main__.TestRAGSystemFunctional) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Statistical Queries ===\n",
      "\n",
      "📝 Query: What is the average price?\n",
      "==================================================\n",
      "\n",
      "📊 RAG Result:\n",
      "1. RAG Data Source: Amazon Products Dataset\n",
      "2. Method: Calculate average price of products in the dataset\n",
      "3. Code: ```python\n",
      "result = df['price'].mean()\n",
      "```\n",
      "4. Result:\n",
      "43.37540368089727\n",
      "\n",
      "✅ Expected Result (Pandas):\n",
      "43.37540368089727\n",
      "Execution Time: 7.87s\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 13.907s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Query: How many unique in the category_name column?\n",
      "==================================================\n",
      "\n",
      "📊 RAG Result:\n",
      "1. RAG Data Source: Amazon Products Dataset\n",
      "2. Method: Counting unique values in category_name column\n",
      "3. Code: ```python\n",
      "result = df['category_name'].nunique()\n",
      "```\n",
      "4. Result:\n",
      "248\n",
      "\n",
      "✅ Expected Result (Pandas):\n",
      "248\n",
      "Execution Time: 6.03s\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_specific_test('test_statistical_queries', TestRAGSystemFunctional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_correlation_queries (__main__.TestRAGSystemFunctional) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Correlation Queries ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 7.321s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Query: What's the correlation between the reviews and isBestSeller columns?\n",
      "==================================================\n",
      "\n",
      "📊 RAG Result:\n",
      "1. RAG Data Source: Amazon product reviews dataset\n",
      "2. Method: Correlation analysis between reviews and isBestSeller columns using pandas corr() function\n",
      "3. Code: ```python\n",
      "cor_matrix = df[['reviews', 'isBestSeller']].corr()\n",
      "result = cor_matrix.loc['reviews','isBestSeller']\n",
      "```\n",
      "4. Result:\n",
      "0.09408527701748222\n",
      "\n",
      "✅ Expected Result (Pandas):\n",
      "0.09408527701745402\n",
      "Execution Time: 7.32s\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_specific_test('test_correlation_queries', TestRAGSystemFunctional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_complex_queries (__main__.TestRAGSystemFunctional) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Complex Queries ===\n",
      "\n",
      "📝 Query: Average price by category_name\n",
      "==================================================\n",
      "\n",
      "📊 RAG Result:\n",
      "1. RAG Data Source: Amazon Products Dataset from Kaggle\n",
      "2. Method: Grouping by category_name and calculating average price\n",
      "3. ```python\n",
      "               result = df.groupby('category_name')['price'].mean()\n",
      "            ```\n",
      "4. Result:\n",
      "category_name\n",
      "Abrasive & Finishing Products                      24.389736\n",
      "Accessories & Supplies                             40.378400\n",
      "Additive Manufacturing Products                    53.659274\n",
      "Arts & Crafts Supplies                             13.458120\n",
      "Arts, Crafts & Sewing Storage                      20.637391\n",
      "                                                     ...    \n",
      "Women's Watches                                    81.703771\n",
      "Xbox 360 Games, Consoles & Accessories             29.766046\n",
      "Xbox One Games, Consoles & Accessories             29.994712\n",
      "Xbox Series X & S Consoles, Games & Accessories    25.657256\n",
      "eBook Readers & Accessories                        34.766684\n",
      "Name: price, Length: 248, dtype: float64\n",
      "\n",
      "✅ Expected Result (Pandas):\n",
      "category_name\n",
      "Abrasive & Finishing Products                      24.389736\n",
      "Accessories & Supplies                             40.378400\n",
      "Additive Manufacturing Products                    53.659274\n",
      "Arts & Crafts Supplies                             13.458120\n",
      "Arts, Crafts & Sewing Storage                      20.637391\n",
      "                                                     ...    \n",
      "Women's Watches                                    81.703771\n",
      "Xbox 360 Games, Consoles & Accessories             29.766046\n",
      "Xbox One Games, Consoles & Accessories             29.994712\n",
      "Xbox Series X & S Consoles, Games & Accessories    25.657256\n",
      "eBook Readers & Accessories                        34.766684\n",
      "Name: price, Length: 248, dtype: float64\n",
      "Execution Time: 7.97s\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 20.260s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Query: Show me all the data where isBestSeller is true and price is more than 1000\n",
      "==================================================\n",
      "\n",
      "📊 RAG Result:\n",
      "1. RAG Data Source: Amazon Products Dataset on Kaggle\n",
      "2. Method: Filter rows where isBestSeller is True and price is greater than 1000\n",
      "3. Code: ```python\n",
      "result = df[(df['isBestSeller'] == True) & (df['price'] > 1000)]\n",
      "```\n",
      "4. Result:\n",
      "              asin                                              title  \\\n",
      "615542  B00UV3LH4Y  Senville LETO Series Mini Split Air Conditione...   \n",
      "630095  B083LMN9FD  Dolphin Nautilus CC Supreme Robotic Pool Vacuu...   \n",
      "\n",
      "                                                   imgUrl  \\\n",
      "615542  https://m.media-amazon.com/images/I/81YIbtYjm1...   \n",
      "630095  https://m.media-amazon.com/images/I/51zNnqVIx3...   \n",
      "\n",
      "                                  productURL  stars  reviews    price  \\\n",
      "615542  https://www.amazon.com/dp/B00UV3LH4Y    4.6        0  1199.99   \n",
      "630095  https://www.amazon.com/dp/B083LMN9FD    4.4        0  1499.00   \n",
      "\n",
      "        listPrice  category_id  isBestSeller  boughtInLastMonth  \\\n",
      "615542        0.0          171          True                500   \n",
      "630095        0.0          196          True                100   \n",
      "\n",
      "                         category_name  \n",
      "615542  Heating, Cooling & Air Quality  \n",
      "630095     Smart Home: Other Solutions  \n",
      "\n",
      "✅ Expected Result (Pandas):\n",
      "              asin                                              title  \\\n",
      "615542  B00UV3LH4Y  Senville LETO Series Mini Split Air Conditione...   \n",
      "630095  B083LMN9FD  Dolphin Nautilus CC Supreme Robotic Pool Vacuu...   \n",
      "\n",
      "                                                   imgUrl  \\\n",
      "615542  https://m.media-amazon.com/images/I/81YIbtYjm1...   \n",
      "630095  https://m.media-amazon.com/images/I/51zNnqVIx3...   \n",
      "\n",
      "                                  productURL  stars  reviews    price  \\\n",
      "615542  https://www.amazon.com/dp/B00UV3LH4Y    4.6        0  1199.99   \n",
      "630095  https://www.amazon.com/dp/B083LMN9FD    4.4        0  1499.00   \n",
      "\n",
      "        listPrice  category_id  isBestSeller  boughtInLastMonth  \\\n",
      "615542        0.0          171          True                500   \n",
      "630095        0.0          196          True                100   \n",
      "\n",
      "                         category_name  \n",
      "615542  Heating, Cooling & Air Quality  \n",
      "630095     Smart Home: Other Solutions  \n",
      "Execution Time: 12.29s\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_specific_test('test_complex_queries', TestRAGSystemFunctional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Robustness Testing**\n",
    "   \n",
    "In this section, test scripts are written based on the previously designed robustness test cases. All test cases are executed sequentially, and the system outputs are compared with the expected results.\n",
    "\n",
    "Below, I have developed a test case class, `TestRAGSystemRobustness`, based on Python's `unittest` framework (`unittest.TestCase`), to systematically perform unit testing on the robustness of the RAG (Retrieval-Augmented Generation) system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestRAGSystemRobustness(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.qa_chain = qa_chain\n",
    "        cls.df = df\n",
    "\n",
    "    def run_test_cases(self, test_cases, category):\n",
    "        print(f\"\\n=== Testing {category} ===\\n\")\n",
    "        \n",
    "        for case in test_cases:\n",
    "            display_query_result(case, self.qa_chain, self.df)\n",
    "            \n",
    "        return None\n",
    "\n",
    "    def test_query_clarity(self):\n",
    "        test_cases = [\n",
    "            \"What's the highest stars in the dataset?\",\n",
    "            \"Show me all the data where isBestSeller is true and price is more than 1000\",\n",
    "            \"Best sellers more than $1000\",\n",
    "            \"Tell me about the DataFrame.\"\n",
    "        ]\n",
    "        self.run_test_cases(test_cases, \"Query Clarity\")\n",
    "\n",
    "    def test_query_complexity(self):\n",
    "        test_cases = [\n",
    "            \"What is the average price?\",\n",
    "            \"When was this dataframe created?\",\n",
    "            \"Show me the top 5 titles with the highest stars and lowest price.\"\n",
    "        ]\n",
    "        self.run_test_cases(test_cases, \"Query Complexity\")\n",
    "\n",
    "    def test_query_exceptions(self):\n",
    "        test_cases = [\n",
    "            \"Hwo mayn culomns?\",\n",
    "            \"What is the weather today?\"\n",
    "        ]\n",
    "        self.run_test_cases(test_cases, \"Query Exceptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_query_clarity (__main__.TestRAGSystemRobustness) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Query Clarity ===\n",
      "\n",
      "\n",
      "==================================================\n",
      "📝 Query: What's the highest stars in the dataset?\n",
      "==================================================\n",
      "Code execution error: 'star_rating'\n",
      "\n",
      "📊 Answer:\n",
      "------------------------------\n",
      "1. RAG Data Source: Kaggle Amazon Products Dataset\n",
      "2. Method: Finding maximum value in 'star_rating' column\n",
      "3. Code: ```python\n",
      "   result = df['star_rating'].max()\n",
      "```\n",
      "4. Result: 5\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "📝 Query: Show me all the data where isBestSeller is true and price is more than 1000\n",
      "==================================================\n",
      "\n",
      "📊 Answer:\n",
      "------------------------------\n",
      "1. RAG Data Source: Amazon product dataset\n",
      "2. Method: Filtered data where isBestSeller is True and price is more than 1000\n",
      "3. Code: ```python\n",
      "   result = df[(df['isBestSeller'] == True) & (df['price'] > 1000)].reset_index()\n",
      "```\n",
      "4. Result:\n",
      "    index        asin                                              title  \\\n",
      "0  615542  B00UV3LH4Y  Senville LETO Series Mini Split Air Conditione...   \n",
      "1  630095  B083LMN9FD  Dolphin Nautilus CC Supreme Robotic Pool Vacuu...   \n",
      "\n",
      "                                              imgUrl  \\\n",
      "0  https://m.media-amazon.com/images/I/81YIbtYjm1...   \n",
      "1  https://m.media-amazon.com/images/I/51zNnqVIx3...   \n",
      "\n",
      "                             productURL  stars  reviews    price  listPrice  \\\n",
      "0  https://www.amazon.com/dp/B00UV3LH4Y    4.6        0  1199.99        0.0   \n",
      "1  https://www.amazon.com/dp/B083LMN9FD    4.4        0  1499.00        0.0   \n",
      "\n",
      "   category_id  isBestSeller  boughtInLastMonth  \\\n",
      "0          171          True                500   \n",
      "1          196          True                100   \n",
      "\n",
      "                    category_name  \n",
      "0  Heating, Cooling & Air Quality  \n",
      "1     Smart Home: Other Solutions  \n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "📝 Query: Best sellers more than $1000\n",
      "==================================================\n",
      "Code execution error: 'name'\n",
      "\n",
      "📊 Answer:\n",
      "------------------------------\n",
      "1. RAG Data Source: Amazon Products Dataset\n",
      "2. Method: Filter products with price > $1000 and count best sellers\n",
      "3. Code: ```python\n",
      "            result = df[df['price'] > 1000]['name'].value_counts().sum()\n",
      "```\n",
      "4. Result: 123\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "📝 Query: Tell me about the DataFrame.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 39.340s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Answer:\n",
      "------------------------------\n",
      "**1. RAG Data Source:** Amazon Products Dataset (Kaggle)\n",
      "\n",
      "**2. Method:** Basic statistics on entire dataset\n",
      "\n",
      "**3. Code:**\n",
      "```python\n",
      "result = df.describe()\n",
      "```\n",
      "\n",
      "**4. Result:\n",
      "              stars       reviews         price     listPrice   category_id  \\\n",
      "count  1.426337e+06  1.426337e+06  1.426337e+06  1.426337e+06  1.426337e+06   \n",
      "mean   3.999512e+00  1.807508e+02  4.337540e+01  1.244916e+01  1.237409e+02   \n",
      "std    1.344292e+00  1.761453e+03  1.302893e+02  4.611198e+01  7.311273e+01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00   \n",
      "25%    4.100000e+00  0.000000e+00  1.199000e+01  0.000000e+00  6.500000e+01   \n",
      "50%    4.400000e+00  0.000000e+00  1.995000e+01  0.000000e+00  1.200000e+02   \n",
      "75%    4.600000e+00  0.000000e+00  3.599000e+01  0.000000e+00  1.760000e+02   \n",
      "max    5.000000e+00  3.465630e+05  1.973181e+04  9.999900e+02  2.700000e+02   \n",
      "\n",
      "       boughtInLastMonth  \n",
      "count       1.426337e+06  \n",
      "mean        1.419823e+02  \n",
      "std         8.362720e+02  \n",
      "min         0.000000e+00  \n",
      "25%         0.000000e+00  \n",
      "50%         0.000000e+00  \n",
      "75%         5.000000e+01  \n",
      "max         1.000000e+05  \n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_specific_test('test_query_clarity', TestRAGSystemRobustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_query_complexity (__main__.TestRAGSystemRobustness) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Query Complexity ===\n",
      "\n",
      "\n",
      "==================================================\n",
      "📝 Query: What is the average price?\n",
      "==================================================\n",
      "\n",
      "📊 Answer:\n",
      "------------------------------\n",
      "1. RAG Data Source: Amazon products dataset\n",
      "2. Method: Calculate average price using pandas mean() function\n",
      "3. Code: ```python\n",
      "result = df['price'].mean()\n",
      "```\n",
      "4. Result:\n",
      "43.37540368089727\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "📝 Query: When was this dataframe created?\n",
      "==================================================\n",
      "Code execution error: 'DataFrame' object has no attribute 'metadata'\n",
      "\n",
      "📊 Answer:\n",
      "------------------------------\n",
      "1. RAG Data Source: Kaggle Amazon Products Dataset\n",
      "2. Method: Extracting creation date information from metadata\n",
      "3. Code: ```python\n",
      "result = df.metadata.iloc[0]['Creation Date']\n",
      "```\n",
      "4. Result: 2023-01-01\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "📝 Query: Show me the top 5 titles with the highest stars and lowest price.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 29.146s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code execution error: 'average_stars'\n",
      "\n",
      "📊 Answer:\n",
      "------------------------------\n",
      "1. RAG Data Source: Kaggle Amazon Products Dataset\n",
      "2. Method: Sort products by average stars and price, then select top 5 titles\n",
      "3. Code:\n",
      "```python\n",
      "result = df.sort_values(by=['average_stars', 'price'], ascending=[False, True])['title'].head(5)\n",
      "```\n",
      "4. Result: ['Product Title1', 'Product Title2', 'Product Title3', 'Product Title4', 'Product Title5']\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_specific_test('test_query_complexity', TestRAGSystemRobustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_query_exceptions (__main__.TestRAGSystemRobustness) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Query Exceptions ===\n",
      "\n",
      "\n",
      "==================================================\n",
      "📝 Query: Hwo mayn culomns?\n",
      "==================================================\n",
      "\n",
      "📊 Answer:\n",
      "------------------------------\n",
      "1. RAG Data Source: Kaggle Amazon Products Dataset\n",
      "2. Method: Counting number of columns in the provided DataFrame 'df'\n",
      "3. Code: ```python\n",
      "   result = len(df.columns)\n",
      "```\n",
      "4. Result:\n",
      "12\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "📝 Query: What is the weather today?\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 19.386s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code execution error: 'product_title'\n",
      "\n",
      "📊 Answer:\n",
      "------------------------------\n",
      "1. RAG Data Source: Kaggle Amazon Products Dataset\n",
      "2. Method: Analyze the dataset for weather information (not applicable since there's no weather data in this dataset)\n",
      "3. Code: ```python\n",
      "result = df['product_title'].head(1).iloc[0]\n",
      "```\n",
      "4. Result: None\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "run_specific_test('test_query_exceptions', TestRAGSystemRobustness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Test Results Analysis\n",
    "\n",
    "Based on the various test results in Section 3.2.2, we can conduct the following analysis:\n",
    "\n",
    "1. **Analysis of Functional Test Case Results**\n",
    "\n",
    "In this section of the tests, the questions asked were relatively basic and clear. The system was generally able to accurately interpret the natural language queries, perform the corresponding DataFrame operations, and return results consistent with expectations. This indicates that the system's basic functionality and accuracy are satisfactory.\n",
    "\n",
    "2. **Analysis of Robustness Test Case Results**\n",
    "\n",
    "The results of this section show that the system is capable of handling more complex queries and returning expected results. It can also intelligently recognize minor spelling errors and provide reasonable answers. For unrelated questions, the system appropriately responds with \"none\" or \"I don’t know\" instead of crashing or returning irrelevant incorrect results. This demonstrates that the system's robustness is adequate.\n",
    "\n",
    "Additionally, the system's output format is consistent and stable, and in cases of errors, it provides specific error messages. This indicates that the system's result readability is satisfactory.\n",
    "\n",
    "However, the system's ability to handle ambiguous queries is still lacking. For some ambiguous queries, the system may fail to correctly interpret the user's intent, leading to incorrect identification of column names. This results in either erroneous output during code execution or errors due to missing corresponding columns.\n",
    "\n",
    "3. **Analysis of Output Stability**\n",
    "\n",
    "In both functional and robustness tests, certain queries yielded inconsistent output results when asked multiple times. While the majority of the outputs were correct, there were occasional instances where incorrect results were produced.\n",
    "\n",
    "4. **Analysis of System Accuracy**\n",
    "   \n",
    "After initializing the runtime environment, the system's accuracy is slightly lower. However, as the number of executions increases, the system's stability and accuracy improve. This reflects the system's ongoing learning and optimization process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison with the Agent Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_pandas_dataframe_agent` is a tool developed by the LangChain team to simplify operations on Pandas DataFrames. This agent can interpret natural language queries from users and convert them into corresponding Pandas operations, enabling data analysis and processing.\n",
    "\n",
    "While the agent is capable of translating user queries into various Pandas operations and automatically generating Pandas code to retrieve and return analytical results, it does have some limitations. For example, it relies on the ability to execute arbitrary code, which may pose security risks, especially when handling untrusted inputs. In such cases, it is recommended to use it within a sandbox environment to mitigate potential risks. Additionally, performance bottlenecks may occur when processing large datasets.\n",
    "\n",
    "Below is an example of using `create_pandas_dataframe_agent` for natural language queries. When creating the agent, the parameter `allow_dangerous_code=True` must be specified to successfully initialize it. In this example, we use the same dataset as the RAG system described in this article and ask three very simple questions. The execution time for these queries varies: the first question is relatively quick, taking around 30 seconds, while the other two take over 1 minute each. In contrast, the RAG system designed in this article processes the same queries in less than 10 seconds each.For slightly more complex queries, there is a high likelihood of parsing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents import create_pandas_dataframe_agent\n",
    "\n",
    "agent = create_pandas_dataframe_agent(\n",
    "    llm,\n",
    "    df,\n",
    "    allow_dangerous_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the average price?', 'output': '$43.38'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "agent.invoke(\"What is the average price?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"What's the highest stars in the dataset?\",\n",
       " 'output': 'Yes, there is at least one product with the highest stars equal to 5.0.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(\"What's the highest stars in the dataset?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How many columns?', 'output': 'There are 12 columns.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(\"How many columns?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Show me all the data where isBestSeller is true and price is more than 1000?',\n",
       " 'output': 'There are two products where `isBestSeller` is True and `price` is greater than 1000:\\n\\n1. Senville LETO Series Mini Split Air Conditioner (asin: B00UV3LH4Y, price: 1199.99)\\n2. Dolphin Nautilus CC Supreme Robotic Pool Vacuum (asin: B083LMN9FD, price: 1499.00)'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agent.invoke(\"Show me all the data where isBestSeller is true and price is more than 1000?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Problems and Improvements\n",
    "\n",
    "Although this system has some advantages over the agent solution, it also has some problems that need to be addressed and improved.\n",
    "\n",
    "### Problems\n",
    "\n",
    "The system's main bottlenecks are as follows:\n",
    "\n",
    "1. It does not work well for fuzzy queries. For example, the result of the fuzzy query \"Best sellers more than $1000\" is unstable, while the result of the explicit query \"Show me all the data where isBestSeller is true and price is more than 1000\" is stable and correct. The system cannot associate the isBestSeller column with Best sellers\n",
    "\n",
    "2. The results are not completely stable: for slightly more complex queries, such as \"Show me the top 5 titles with the highest stars and lowest price.\", the answers are sometimes correct and sometimes incorrect.\n",
    "\n",
    "3. It is unable to identify which questions do not require code execution, resulting in redundant operations. For example, the question \"When was this dataframe created?\" can be answered directly from the vector database without the need for pandas operations. Executing the code will instead result in an error because the dataframe itself does not have relevant information. However, the current system cannot directly skip the code execution step. Although I have set the system to directly return the results of the qa_chain if the code execution reports an error, ensuring the correctness of the results, it is still defective in terms of system operation efficiency.\n",
    "\n",
    "### Proposed Improvements\n",
    "\n",
    "To solve these problems, we may be able to make improvements in the following two directions:\n",
    "\n",
    "1. Continue to improve the existing code, for example, we can continue to improve the content of the prompt variable in the create_qa_chain function, give the rag system clearer and more specific instructions to avoid confusion; and improve the judgment conditions before executing the pandas code operation in the query_dataframe function, so that the system can effectively filter unnecessary code execution.\n",
    "\n",
    "2. Try to use other llms and embedding models to see if these problems can be solved.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
