{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple natural language dataframe question-answering system, which is based on the LangChain framework and combines OllamaLLM's Llama 3.1 LLM, Chroma vector database and HuggingFace's all-MiniLM-L6-v2 Embedding model. This allows the system to be lightweight, localized and flexible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment preparation\n",
    "\n",
    "These are the packages and modules needed to run this system. I have already installed them and will not run them here.\n",
    "\n",
    "```bash\n",
    "pip install pandas langchain langchain-ollama langchain-chroma langchain-huggingface\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required Python libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- `pd`: DataFrame operations and data analysis\n",
    "- `Chroma`: Vector database for storing and searching embeddings\n",
    "- `Document`: Container for text content and metadata\n",
    "- `OllamaLLM`: Interface for local LLM models\n",
    "- `PromptTemplate`: Template builder for LLM inputs\n",
    "- `StrOutputParser`: Converts LLM outputs to strings\n",
    "- `RunnablePassthrough`:Pass the user's question directly to subsequent processing steps\n",
    "- `HuggingFaceEmbeddings`: A bridge to make Hugging Faceâ€™s embedding capabilities easier to use in the LangChain environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 1: Data preparation\n",
    "#### step1: Load data\n",
    "\n",
    "Load csv data, the file is downloaded from kaggle: https://www.kaggle.com/datasets/spscientist/students-performance-in-exams\n",
    "\n",
    "Only 20 rows of data are randomly selected to facilitate observation of model effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('random_sample.csv') \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2: store the column name information in the vector database as embeddings \n",
    "\n",
    "Dataframe is structured data. Each row of data has the same structure and column name. If all dataframe information is stored in the vector database, the retrieval effect will be reduced due to a large amount of identical structural information.\n",
    "\n",
    "To solve this, the key task of this RAG system is not to directly return the dataframe query result to the user. Instead, it converts the user's question into corresponding Pandas query code, and Python executes the code to get the final result.\n",
    "\n",
    "For generating Pandas code, only the column name information is needed. So, we only store the column names in the vector database to allow efficient similarity search later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a structured document object for embedding in the next step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'type': 'column_info'}, page_content=\"The column names of the dataset are: ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course', 'math score', 'reading score', 'writing score'].\")]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "col_info = f\"The column names of the dataset are: {df.columns.tolist()}.\"\n",
    "\n",
    "documents = [Document(page_content=col_info, metadata={\"type\": \"column_info\"})]\n",
    "\n",
    "documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Embed document content and store it in the vector database\n",
    "\n",
    "This is done to enable efficient semantic search, so the system can quickly find documents related to the user's question.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2: Create a RAG question-answering system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create a Retriever\n",
    "\n",
    "A vector database is a database that stores embedding vectors. It also works as a retriever to find the embedded documents most similar to the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k: Amount of documents to return, default is 4, there is only 1 document in my document, so set k=1\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create an Augmentor\n",
    "\n",
    "The augmentor is used to improve the retriever's performance. It combines the retrieved document content, the user's question, and the rules for generating pandas code. This input is sent to the LLM, helping it better understand the user's intent and generate more accurate pandas query code.\n",
    "\n",
    "Since our vector database stores very little information (only column names), setting up the augment information is very important. I created a clear and detailed prompt for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a dataframe analysis assistant. Provide concise answers with only 1 sections:\n",
    "\n",
    "               Code: [write one line of pandas code in ```python``` block, \n",
    "                        EXAMPLE:code:\n",
    "                                ```python\n",
    "                                    df['category_name'].value_counts().count()\n",
    "                                ```\n",
    "                        ]\n",
    "\n",
    "\n",
    "                Context:{context}\n",
    "                Question: {question}\n",
    "                \n",
    "                Note: Use the existing DataFrame 'df' provided by the system, do not create or read a new one.\n",
    "                \n",
    "                You can use these pandas operations:\n",
    "                1. Basic statistics: df.describe(), df[column].mean(), df[column].max(), etc.\n",
    "                2. Group statistics: df.groupby(column).agg()\n",
    "                3. Sorting: df.sort_values(by=column)\n",
    "                4. Filtering: df[df[column] > value]\n",
    "                ...\n",
    "                \n",
    "                Rules:\n",
    "                1. No explanations or additional text.\n",
    "                2. write complete executable code in ```python``` block using the existing 'df'.\n",
    "                3. Use proper column names from the DataFrame.\n",
    "\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step3: Create a Generator\n",
    "\n",
    "Now, use llm to generate pandas query code based on the retrieved and augmented information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1\", temperature=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4: Create a chain\n",
    "\n",
    "Use pipeline operations to connect the retriever, augmentor, and generator in series to form a complete rag question-answering system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "        {\"context\": retriever,\"question\": RunnablePassthrough()}\n",
    "        | PROMPT\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part3: Result execution\n",
    "\n",
    "Execute the pandas code returned by rag locally and return the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_and_execute(answer): \n",
    "       \n",
    "    # Extract code block\n",
    "    code_start = answer.find(\"```python\") + 9\n",
    "    code_end = answer.find(\"```\", code_start)\n",
    "    code = answer[code_start:code_end].strip()\n",
    "    \n",
    "    # Create local namespace and execute code\n",
    "    local_dict = {'df': df, 'pd': pd}\n",
    "    code = f\"result = {code}\"\n",
    "    exec(code, None, local_dict)\n",
    " \n",
    "    return local_dict.get('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_dataframe(question: str):\n",
    "    \n",
    "    code = chain.invoke(question)\n",
    "    result = extract_code_and_execute(code)\n",
    "    \n",
    "    return {'code': code, 'result': result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': \"```python\\ndf['math score'].max()\\n```\", 'result': 91}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the highest math score?\"\n",
    "query_dataframe(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': \"```python\\ndf['reading score'].min()\\n```\", 'result': 26}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the lowest reading score?\"\n",
    "query_dataframe(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reading score'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
